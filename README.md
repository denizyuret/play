# research

TODO
* Implement RAM versions of basic image utils, bubble graph
* Use tanh(alpha*x) for soft binarization
* Use separate learnt masks for three-value -1,0,1 networks
* RNN - residual connections
* RNN - reversible units

* finish resnet implementation - done
* look at mobilenet, shufflenet, efficientnet (best)
* do multiheadattn layer for shadi
* test bwn on cifar, mini-imagenets etc with resnet
* figure out xnor
* try tanh idea, regularization idea etc

EfficientNet:
https://arxiv.org/abs/1905.11946
http://cs231n.stanford.edu/slides/2020/lecture_9.pdf
https://www.dlology.com/blog/transfer-learning-with-efficientnet/
https://keras.io/api/applications/efficientnet/
https://keras.io/examples/vision/image_classification_efficientnet_fine_tuning/
https://github.com/qubvel/efficientnet
https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet
https://analyticsindiamag.com/implementing-efficientnet-a-powerful-convolutional-neural-network/
https://hackmd.io/@bouteille/HkH1jUArI
